---
title       : Spam Filtering
subtitle    : Using Machine Learning for E-Mail Content Analysis
author      : Milan Fort
job         : 17.07.2016
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides


--- {class: slide1, id: id1, bg: white}
## Introduction

```{r, echo=FALSE, results='hide', message=TRUE, cache=FALSE}
set.seed(99)
options(digits=2)
url <- "http://www.milanfort.com/spam-filtering/data/email.txt"
emails <- read.table(url, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
```

* This is our final project for the _Developing Data Products_ online course on Coursera
* Task was to develop an interactive web application using Shiny, a web framework for R
* The application must support user input and perform arbitrary analysis interactively
* We chose to build a prediction model for classifying e-mail messages as spam or ham
* We employed the Logistic Regression [4] machine learning algorithm
* We used the dataset from [3] containing **`r nrow(emails)`** e-mail messages with
**`r ncol(emails) - 1`** different variables


--- &twocols widthLeft:66% widthRight:33%
## Model Building and Evaluation

```{r, echo=FALSE, results='hide', message=TRUE, cache=FALSE}
options(digits=2)
selectedVariables <- c(
    "spam", "to_multiple", "cc", "attach", "dollar", "winner",
    "inherit", "password", "format", "re_subj", "exclaim_subj"
)
emails <- emails[selectedVariables]

convertToIndicator <- function(value) {
    factor(ifelse(value > 0, "yes", "no"), levels = c("yes", "no"), ordered = TRUE)
}

# Turn winner temporarily into number in order for the next conversion to work properly
emails["winner"] <- ifelse(emails["winner"] == "yes", 1, 0)
emails <- as.data.frame(apply(emails, 2, convertToIndicator))

train <- sample(c(TRUE, FALSE), size = nrow(emails), replace = TRUE, prob = c(0.6, 0.4))
fit <- glm(spam~to_multiple+attach+winner+format+re_subj, data = emails, family = binomial, subset = train)
testData <- emails[!train, ]
prediction <- predict(fit, newdata = testData, type="response")
threshold <- 0.5
prediction <- ifelse(prediction >= threshold, "yes", "no")
accuracy <- mean(prediction == as.character(testData[, "spam"]))
confusionMatrix <- table(prediction, as.character(testData[, "spam"]))
```


* We used approximately 60% of the data for model training and 40% for testing
* We selected final model using *backward-elimination* based on *p-values* with 0.01 cutoff point
* The model evaluation depends on the selection of spam probability threshold
* Using spam probability threshold **`r threshold`**, we get the following results:

*** =left

- Number of test e-mail messages: **`r nrow(testData)`**
- Number of correctly classified messages:
**`r confusionMatrix[1, 1] + confusionMatrix[2, 2]`**
- Number of messages correctly classified as spam: **`r confusionMatrix[2, 2]`**
- Number of messages correctly classified as ham: **`r confusionMatrix[1, 1]`**
- Prediction accuracy: **`r accuracy`**
- Test error rate: **`r 1 - accuracy`**


*** =right

```{r, echo=TRUE, results='markup', message=TRUE, cache=FALSE}
confusionMatrix
```


---
## Conclusion

* The fitted prediction model serves well as a foundation for our web application [2]
* The application allows to configure e-mail properties, adjust spam probability threshold, and predicts the spam probability of the specified e-mail
* The accuracy of the prediction model is not sufficient for a real-world spam filter
* We suggest the following potential improvements:
    * Use more numerical predictors, transformed via log-transformation to eliminate outliers
    * Create new predictors based on the history of the e-mail account (see [1] for details)
    * Evaluate other machine learning algorithms, such as Bayes classifiers [4]


---
## References

1. Fort M.
Spam Filtering: Using Machine Learning for E-Mail Content Classification.
Data analysis report. 2016. Available at
[http://www.milanfort.com/spam-filtering/](http://www.milanfort.com/spam-filtering/).

2. Fort M.
E-Mail Spam Filter Test.
Interactive web application.
Deployed at
[https://milanfort.shinyapps.io/spam-filtering/](https://milanfort.shinyapps.io/spam-filtering/).
Source code available at
[https://github.com/milanfort/spam-filtering/](https://github.com/milanfort/spam-filtering/).

3. Diez D.; Barr C.; Cetinkaya-Rundel M.
OpenIntro Statistics, Second Edition.
ISBN 9781478217206. CreateSpace Independent Publishing Platform, 2012.

4. James G.; Witten D.; Hastie T.; Tibshirani R. 
An Introduction to Statistical Learning: with Applications in R.
Springer Texts in Statistics. ISBN 9781461471370. Springer, 2013.


<!-- --- Notes --- -->
<!-- library(slidify) -->
<!-- slidify("slides.Rmd") -->
<!-- x squared: $x^2$ -->
<!-- centered formulae: $$\frac{-b \pm \sqrt{b^2 - 4 a c}}{2a}$$ -->

<style>
.title-slide {
    background-color: white;
}

.title-slide hgroup > h1,
.title-slide hgroup > h2 {
    color: #2F4F4F;
}

.title-slide hgroup p {
    color: #444444;
}

article {
    margin-top: 50px;
}

slides > slide > hgroup + article {
    margin-top: 50px;
}

strong {
    font-weight: bold;
}

em {
    font-style: italic;
}

ul {
    list-style-type: square;
}

li ul {
    list-style-type: disc;
}

div.centered ul {
    padding-left: 45px;
    list-style-type: disc;
}
</style>
