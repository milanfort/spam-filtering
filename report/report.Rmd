---
title: "Spam Filtering: Using Machine Learning for E-Mail Content Classification"
author: "Milan Fort"
date: "14.07.2016"
geometry: 1cm
output:
  html_document:
    theme: cerulean
---

```{r echo=FALSE}
set.seed(99)
options(digits=4)
```

### Introduction

In this data analysis report we will build a prediction model for classifying e-mail messages as spam or not spam (ham). The goal is to create a prediction model that can be later transformed into a simple data product - 
an application that accepts some user input and uses machine learning techniques to produce corresponding output.

This data analysis report is the foundation of our final project for the
[Developing Data Products](https://www.coursera.org/learn/data-products/) online course.
It is based on the e-mail spam filtering discussion from Chapter 8 of OpenIntro Statistics textbook [1].
As such, it can be seen as a companion, practical implementation in R of the discussed topic.

### Data Cleaning and Exploratory Data Analysis

We will use the *email* dataset from
[OpenIntro Statistics Extras](https://www.openintro.org/stat/extras.php) website, which we have re-deployed at our site for direct access.
```{r cache=FALSE}
url <- "../data/email.txt"
emails <- read.table(url, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
```
The dataset consists of **`r nrow(emails)` e-mail messages**, where each message is classified
as spam or ham (in variable *spam*, where value *1* corresponds to spam) and 
has **`r ncol(emails) - 1` other characteristics** recorded.
```{r}
names(emails)
```
Some of these variables, such as *time*, are not useful for classifying e-mails as spam and we can safely remove them. For simplicity, we will also remove some of the numerical variables, such as *num_char* or *line_breaks*. These variables could be useful for prediction, but might contain outliers which would affect the accuracy of the final model unless they are further cleaned and transformed [1].
```{r}
selectedVariables <- c(
    "spam", "to_multiple", "cc", "attach", "dollar", "winner",
    "inherit", "password", "format", "re_subj", "exclaim_subj"
)
emails <- emails[selectedVariables]
names(emails)
summary(emails)
```
To simplify the resulting user interface of our data product even further,
we will convert all remaining variables into indicator variables, which denote
whether the given characteristic is present in the message or not.
```{r}
convertToIndicator <- function(value) {
    factor(ifelse(value > 0, "yes", "no"), levels = c("yes", "no"), ordered = TRUE)
}

# Turn winner temporarily into number in order for the next conversion to work properly
emails["winner"] <- ifelse(emails["winner"] == "yes", 1, 0)
emails <- as.data.frame(apply(emails, 2, convertToIndicator))

summary(emails)
```
The dataset has now **`r ncol(emails)` variables**. The meaning of these variables is summarized in the table below [1]:

Variable     | Description
--------     | -----------------------------------------------------------------
spam         | Specifies whether the message was classified as spam
to_multiple  | Indicates whether the message had multiple recipients in the To field
cc           | Indicates whether the message had someone in the CC field
attach       | Indicates whether the message had an attachment
dollar       | Indicates whether the message contained the dollar symbol or the word "dollar"
winner       | Indicates whether the message contained the word "winner"
inherit      | Indicates whether the message contained the word "inherit" or its variations
password     | Indicates whether the message contained the word "password"
format       | Indicates whether the message was in HTML format
re_subj      | Indicates whether the message had "Re:" at the beginnig of its subject
exclaim_subj | Indicates whether the message contained an exclamation mark in its subject


### Model Building and Evaluation

We will split the **`r nrow(emails)` emails** randomly into *training* and *testing* subsets, where each 
subset contains approximately 60% and 40% of data, respectively.
```{r}
train <- sample(c(TRUE, FALSE), size = nrow(emails), replace = TRUE, prob = c(0.6, 0.4))
```

We can now proceed to model building. Since the outcome is a categorical variable with only two levels
(spam or ham), we will use 
[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression).
We will start with a model that includes all predictors (so-called *full model*).
```{r}
fit <- glm(spam~to_multiple+cc+attach+dollar+winner+inherit+password+format+re_subj+exclaim_subj, data = emails, family = binomial, subset = train)
summary(fit)
```
As we can see from the *p-values* (last column in the coefficients section), not all predictors are contributing significantly to the model, i.e. for some predictors, there is no evidence of association between the predictor and the outcome. Therefore, we will use *backward-elimination* to create a smaller model with only statistically significant p-values (we will use 0.01 as cutoff). In this multi-step process, we eliminate the predictor with the largest p-value, and refit the model with the remaining predictors, until all predictors are significant. The final model, which is the result of this process, is shown below.
```{r}
fit <- glm(spam~to_multiple+attach+winner+format+re_subj, data = emails, family = binomial, subset = train)
summary(fit)
```
We will now test the prediction accuracy of this model on the testing subset of the data.
```{r}
testData <- emails[!train, ]
prediction <- predict(fit, newdata = testData, type="response")
```
The `type="response"` argument causes the predict function to return the predicted probabilities of the outcome variable, i.e. the probability that an e-mail is spam.

We now need to determine a probability cutoff point (or *threshold*) -- a minimum probability for spam.
Setting this threshold too low would result in too many good e-mails classified as spam (*false positives*).
On the other hand, setting it too high would mean that too many spam messages are not classified as spam
(*false negatives*).

In practice, such threshold could be somewhere between 0.95 and 0.99. Our data product will support the selection of the threshold as part of the user interface.
For this evalution, we will set the threshold to 0.5.
```{r}
threshold <- 0.5
prediction <- ifelse(prediction > threshold, "yes", "no")
accuracy <- mean(prediction == as.character(testData[, "spam"]))
(confusionMatrix <- table(prediction, as.character(testData[, "spam"])))
```
The *confusion matrix* above has correct predictions on the diagonal. Thus, our model correctly
classified **`r confusionMatrix[1, 1]`** messages as ham, and **`r confusionMatrix[2, 2]`** messages as spam.
The overall prediction accuracy of the final model on the testing data is **`r accuracy`**,
which implies that the *test error rate* is **`r 1 - accuracy`**.

For comparison, we will now create a naive prediction model, which classifies every message as ham,
and compute its accuracy.
```{r}
naivePrediction <- rep("no", times = length(prediction))
(naiveAccuracy <- mean(naivePrediction == as.character(testData[, "spam"])))
```
As we can see, the accuracy of our model fitted using logistic regression is only slightly better than the accuracy of the naive model. We will discuss the potential model improvements later in Section "Conclusion and Future Work".


### Practical Model Application

We can now use the fitted model to classify a particular e-mail message described by the user of the data product. For example, let's assume the e-mail has the following properties:

* It contains an attachment
* It contains the the word "winner"
* It is in HTML format
```{r}
emailProperties <- c("attach", "winner", "format")
testEmail <- rep(factor("no", levels = c("yes", "no"), ordered = TRUE), ncol(emails))
names(testEmail) <- names(emails)
testEmail[emailProperties] <- "yes"
testEmail <- as.data.frame(t(as.matrix(testEmail)))
(result <- predict(fit, newdata = testEmail, type = "response"))
```
Hence, the model predicted that the specified e-mail message is a **`r ifelse(result > threshold, "SPAM", "HAM")`** under the threshold **`r threshold`**.


### Conclusion and Future Work

This data analysis report laid the foundation for our data product - 
a simple [interactive e-mail classifier](http://www.shinyapps.io/app)
implemented using R and [Shiny](http://shiny.rstudio.com/).

The accuracy of the prediction model, however, is not sufficient for a real-world spam filter.
There are number of improvements that we could employ to improve the performance of the model [1]:

* Transform the numerical predictors using log-transformation to eliminate outliers and use
for model building directly (not just as indicator variables).

* Add additional variables, such as an indicator whether there was prior two-way communication with the mesage's sender, or indicator variables that rely on previously classified e-mails, such as known spam-sender,
or known spam e-mail content (links, phrases, etc.).

Finally, other machine learning algorithms, such as
[Naive Bayes classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
could be also investigated to see if they have better performance than logistic regression used in this analysis.


### References

1. Diez D.; Barr C.; Ã‡etinkaya-Rundel M.
OpenIntro Statistics, Second Edition.
ISBN 9781478217206. CreateSpace Independent Publishing Platform, 2012.

2. James G.; Witten D.; Hastie T.; Tibshirani R. 
An Introduction to Statistical Learning: with Applications in R.
Springer Texts in Statistics. ISBN 9781461471370. Springer, 2013.
